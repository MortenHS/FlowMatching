{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import minari\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stat\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torch cuda version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    observations = [torch.as_tensor(x.observations['observation']) for x in batch]\n",
    "    lengths = torch.tensor([obs.shape[0] for obs in observations])  # Tensor of original lengths\n",
    "\n",
    "    # Pad with zeros first\n",
    "    padded_obs = torch.nn.utils.rnn.pad_sequence(observations, batch_first=True)\n",
    "\n",
    "    # Use advanced indexing to replace padding with the last valid value\n",
    "    mask = torch.arange(padded_obs.shape[1]).expand(len(batch), -1) >= lengths.unsqueeze(1)\n",
    "    padded_obs[mask] = padded_obs[torch.arange(len(batch)), lengths - 1].unsqueeze(1).expand_as(padded_obs)[mask]\n",
    "\n",
    "    return {\n",
    "        \"observations\": padded_obs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = minari.load_dataset('D4RL/pointmaze/umaze-v2')\n",
    "print(f\"Total amount of episodes: {dataset.total_episodes}\")  # 13210 episodes\n",
    "batch_size = 64\n",
    "\n",
    "dataLoader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "for batch in dataLoader:\n",
    "    test_batch = batch['observations']\n",
    "    print(f\"test_batch.shape: {test_batch.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset trajectory example\n",
    "* One episode\n",
    "* Based on the 'Observation' parameter of Observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette(\"flare\", len(test_batch))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(len(test_batch) - 1): # Traj plot\n",
    "    plt.plot(test_batch[0, i:i+2, 0].cpu().numpy(), test_batch[0, i:i+2, 1].cpu().numpy(), color=colors[i], marker='o')\n",
    "plt.title('Positions in Trajectory')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLOW MATCHING\n",
    "\n",
    "* TODO: Se på x_t uttrykket, og hva funksjonen tar inn\n",
    "* Sjekk sampling output (shape)\n",
    "* Sjekk at loss går nedover med trening\n",
    "* Skriv i overleaf hva treningsfunksjon skal være, og sampling prosedyre\n",
    "* Så implementer loss beregning og sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryFlowModel(nn.Module):\n",
    "    def __init__(self, obs_dim, hidden_dim=128, num_layers=5):\n",
    "        \"\"\"\n",
    "        A neural network that estimates the velocity field for flow matching.\n",
    "        \n",
    "        Args:\n",
    "            obs_dim (int): Dimensionality of observations (D_obs)\n",
    "            hidden_dim (int): Number of hidden units in the MLP\n",
    "            num_layers (int): Number of layers in the MLP\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        input_dim = obs_dim + 1  # We include time `t` as an input\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim  # Keep hidden layer size consistent\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_dim, obs_dim))  # Output has the same shape as observations\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Forward pass for the trajectory flow model.\n",
    "        \"\"\"\n",
    "        # print(f\"t.shape in model code: {t.shape}\")\n",
    "        sec_dim = x.shape[1]\n",
    "        t_expanded = t[:, None, None].expand(-1, sec_dim, -1)\n",
    "        xt = torch.cat([x, t_expanded], dim=-1).to(device, dtype=torch.float32)\n",
    "        velocity = self.network(xt)  # Predict flow field\n",
    "\n",
    "        return velocity  # Shape: (N, T, D_obs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 3e-3 # Learning rate\n",
    "\n",
    "traj_dim = 4 # [xpos, ypos, xvel, yvel]\n",
    "flow_model = TrajectoryFlowModel(traj_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(flow_model.parameters(), lr=lr)\n",
    "\n",
    "xt_prog = []\n",
    "losses = []\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Training Progress\"): # Se https://jmtomczak.github.io/blog/18/18_fm.html for kobling til teorien\n",
    "    for batch in dataLoader:\n",
    "        observations = batch['observations']\n",
    "        x1 = observations[torch.randint(observations.size(0), (batch_size, ))].to(device)\n",
    "        x0 = torch.rand_like(x1).to(device)\n",
    "        vel_target = x1 - x0 # dxt\n",
    "        t = torch.rand((x1.size(0),), device=device)\n",
    "        xt = t[:, None, None] * x1 + (1 - t[:, None, None]) * x0 # See eq. (2.3) i guide and code s.5\n",
    "        xt_prog.append(xt)\n",
    "        prediction = flow_model(xt, t)\n",
    "        loss = ((vel_target - prediction)**2).mean()\n",
    "        loss = loss.to(device, dtype=torch.float32)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_lst = []\n",
    "for ver in xt_prog:\n",
    "    ver = ver.cpu().numpy()\n",
    "    xt_lst.append(ver)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(xt_lst[0][:, 0], xt_lst[0][:, 1], label='xt', marker='o')\n",
    "plt.title('xt Progression')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_in_pairs(save_dir):\n",
    "    \"\"\"\n",
    "    Display images with filenames matching 'sampling_step_*.png' in natural order, two at a time.\n",
    "\n",
    "    Args:\n",
    "        save_dir (str): Directory containing the images.\n",
    "    \"\"\"\n",
    "    image_files = natsorted(glob.glob(os.path.join(save_dir, \"sampling_step_*.png\")))\n",
    "\n",
    "    for i in range(0, len(image_files), 2):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        for j in range(2):\n",
    "            if i + j < len(image_files):\n",
    "                img = plt.imread(image_files[i + j])\n",
    "\n",
    "                plt.subplot(1, 2, j + 1)\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')\n",
    "                plt.title(f\"Step {image_files[i + j].split('_')[-1].split('.')[0]}\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ODE Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ODE Solver w/ Midpoint method\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "class FlowMatchingODEFunc(torch.nn.Module):\n",
    "    def __init__(self, flow_model):\n",
    "        super().__init__()\n",
    "        self.flow_model = flow_model\n",
    "\n",
    "    def forward(self, t, x_t):\n",
    "        \"\"\"\n",
    "        Compute the velocity field at time t for input x_t.\n",
    "        t: Scalar tensor (shape: []) representing time.\n",
    "        x_t: Tensor of shape (batch_size, sequence_length, dim) representing the state.\n",
    "        Returns: Velocity field of shape (batch_size, sequence_length, dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, dim = x_t.shape\n",
    "        t_expanded = t.expand(batch_size)  # Expand t to match batch size\n",
    "        velocity_pred = self.flow_model(x_t, t_expanded)  # Predict velocity at (x_t, t)\n",
    "        return velocity_pred  # dx/dt = velocity field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectories(flow_model, xt_ode, ode_method, sampling_steps_ode, save_dir_ode, plot_every_ode):\n",
    "    \"\"\"\n",
    "    Uses an ODE solver to sample trajectories from the learned flow model.\n",
    "    \n",
    "    Parameters:\n",
    "    - flow_model: The trained flow matching model.\n",
    "    - xt_ode: Initial noise tensor (batch_size, seq_len, dim).\n",
    "    - sampling_steps_ode: Number of integration steps.\n",
    "    - save_dir_ode: Directory to save visualization images.\n",
    "    - plot_every_ode: Interval for plotting intermediate steps.\n",
    "    \n",
    "    Returns:\n",
    "    - sample_trajs: The final sampled trajectories.\n",
    "    \"\"\"\n",
    "    device = xt_ode.device\n",
    "    flow_model.eval().requires_grad_(False)\n",
    "\n",
    "    # Define the ODE function\n",
    "    ode_func = FlowMatchingODEFunc(flow_model).to(device)\n",
    "\n",
    "    # Time grid from 0 to 1\n",
    "    t_eval = torch.linspace(0, 1, sampling_steps_ode, device=device)\n",
    "\n",
    "    # Solve ODE using `odeint`\n",
    "    sample_trajs = odeint(ode_func, xt_ode, t_eval, method=ode_method)  # Shape: (sampling_steps_ode, batch_size, seq_len, dim)\n",
    "\n",
    "    # Extract final sampled trajectories\n",
    "    final_samples = sample_trajs[-1]  # Get the last time step\n",
    "\n",
    "    # Plot intermediate snapshots\n",
    "    snapshots_samples = []\n",
    "    for i in range(0, sampling_steps_ode, plot_every_ode):\n",
    "        snapshots_samples.append(sample_trajs[i].clone())  # Store for visualization\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for observations in observation_lst:\n",
    "            plt.plot(observations[0, :, 0].cpu().numpy(), observations[0, :, 1].cpu().numpy(), color='g', alpha=0.5)\n",
    "\n",
    "        plt.scatter(sample_trajs[i, 0, :, 0].cpu().numpy(), sample_trajs[i, 0, :, 1].cpu().numpy(), \n",
    "                    color='r', label=f\"Step {i}\")\n",
    "        \n",
    "        plt.title(f'Sampling Progression ODE (Step {i})')\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "        save_path = os.path.join(save_dir_ode, f\"sampling_step_ode_{i}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "    flow_model.train().requires_grad_(True)\n",
    "    print(f\"Finished sampling with: {ode_method}\")\n",
    "    \n",
    "    return final_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_steps_ode = 151\n",
    "save_dir_ode = \"Sampling_Plots_ODE\"\n",
    "os.makedirs(save_dir_ode, exist_ok=True)\n",
    "plot_every_ode = 15\n",
    "# Available methods: [\"dopri8\", \"dopri5\", \"bosh3\", \"fehlberg2\", \"adaptive_heun\", \"euler\", \"midpoint\", \"rk4\", \"explicit_adams\", \"implicit_adams\", \"fixed_adams\", \"scipy_solver\"]\n",
    "ode_method = 'dopri5'\n",
    "\n",
    "observation_lst = []\n",
    "max_batches = 100\n",
    "for count, batch in enumerate(dataLoader):\n",
    "    if count == max_batches:\n",
    "        break\n",
    "    obs = batch['observations']\n",
    "    observation_lst.append(obs)\n",
    "\n",
    "xt_ode = torch.randn_like(xt)\n",
    "final_trajs_ode = sample_trajectories(flow_model, xt_ode, ode_method, sampling_steps_ode, save_dir_ode, plot_every_ode)\n",
    "display_images_in_pairs(save_dir_ode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "save_dir = \"Sampling_Plots\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "sampling_steps = 200 # Seems to not matter how many steps I take past this point.\n",
    "plot_every = 20\n",
    "\n",
    "snapshots_samples = []\n",
    "\n",
    "sample_trajs = torch.randn_like(xt) # 64 long\n",
    "flow_model.eval().requires_grad_(False)\n",
    "\n",
    "for i, t in enumerate(torch.linspace(0, 1, sampling_steps, device=device), start = 1):\n",
    "    pred = flow_model(sample_trajs, t.expand(xt.size(0)))\n",
    "    sample_trajs = sample_trajs + (1 / sampling_steps) * pred\n",
    "    \n",
    "    if i % plot_every == 0:\n",
    "        snapshots_samples.append(sample_trajs.clone())\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        for observations in observation_lst:\n",
    "            plt.plot(observations[0, :, 0].cpu().numpy(), observations[0, :, 1].cpu().numpy(), color='g', alpha=0.5)\n",
    "\n",
    "        plt.scatter(sample_trajs[0, :, 0].cpu().numpy(), sample_trajs[0, :, 1].cpu().numpy(), color='r', label=\"Sampled trajectories\")\n",
    "        plt.title('Sampling progression')\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "        save_path = os.path.join(save_dir, f\"sampling_step_{i}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        \n",
    "        plt.close()\n",
    "\n",
    "flow_model.train().requires_grad_(True)\n",
    "print(\"Finished sampling\")\n",
    "# print(sample_trajs)\n",
    "display_images_in_pairs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = sum(losses)/len(losses)\n",
    "med_loss = stat.median(losses)\n",
    "\n",
    "save_dir = 'Plotted loss'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "plot_path = os.path.join(save_dir, 'loss_plot.png')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, label='Loss', marker='o')\n",
    "plt.axhline(y=avg_loss, color='r', linestyle='--', label=f'Average Loss: {avg_loss:.4f}')\n",
    "plt.axhline(y=med_loss, color='y', linestyle='--', label=f'Median Loss: {med_loss:.4f}')\n",
    "plt.title('Loss Progression')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(plot_path)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
